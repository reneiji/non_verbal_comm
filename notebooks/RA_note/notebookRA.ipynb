{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a15174e",
   "metadata": {},
   "source": [
    "Renan's notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f9899b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom PIL import Image\\nimport pillow_avif  # noqa: F401, needed to register AVIF support\\n\\nimg = Image.open(\"/Users/renan/code/reneiji/non_verbal_comm/serious-young-african-man.avif\")\\nimg = img.convert(\"RGB\")\\nimg.save(\"serious-young-african-man.jpg\", \"JPEG\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment the following if converting from AVIF to JPEG\n",
    "\n",
    "'''\n",
    "from PIL import Image\n",
    "import pillow_avif  # noqa: F401, needed to register AVIF support\n",
    "\n",
    "img = Image.open(\"/Users/renan/code/reneiji/non_verbal_comm/serious-young-african-man.avif\")\n",
    "img = img.convert(\"RGB\")\n",
    "img.save(\"serious-young-african-man.jpg\", \"JPEG\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c93751f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64debfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0.03257695255349397, 'disgust': 1.1144623255532201e-11, 'fear': 0.002219444749328375, 'happy': 99.84859825985453, 'sad': 0.0025796688186143445, 'surprise': 0.011756884540774653, 'neutral': 0.10226941612503933}\n"
     ]
    }
   ],
   "source": [
    "# Analyze an image (local path or URL)\n",
    "result = DeepFace.analyze(img_path = \"/Users/renan/code/reneiji/non_verbal_comm/notebooks/RA_note/threatening_smile.jpeg\", actions = ['emotion'])\n",
    "\n",
    "print(result[0]['emotion'])  # prints a dictionary with emotion scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c21709b",
   "metadata": {},
   "source": [
    "## This is the results for cropping mouth and eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a7a12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘€ Eye Region Emotions: {'angry': 2.430010193066323e-07, 'disgust': 3.8639415869941882e-19, 'fear': 0.0011287221241218504, 'happy': 0.0009147149285126943, 'sad': 2.7052702092356107e-09, 'surprise': 2.3790910647775165e-10, 'neutral': 99.99796152114868}\n",
      "ðŸ‘„ Mouth Region Emotions: {'angry': 0.005284247065553092, 'disgust': 0.0018803059414999191, 'fear': 35.75203248882681, 'happy': 0.002038035988452478, 'sad': 51.42822282650743, 'surprise': 4.560845401032683e-06, 'neutral': 12.810538746182768}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748246920.924378 2503399 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M3 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1748246920.926493 2504185 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748246920.933752 2504186 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748246920.937409 2504184 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from deepface import DeepFace\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "\n",
    "img_path = \"/Users/renan/code/reneiji/non_verbal_comm/notebooks/serious-young-african-man.jpg\"\n",
    "img = cv2.imread(img_path)\n",
    "if img is None:\n",
    "    print(f\"âš  Failed to load image. Check path: {img_path}\")\n",
    "    exit()\n",
    "h, w, _ = img.shape\n",
    "rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "results = face_mesh.process(rgb_img)\n",
    "\n",
    "if results.multi_face_landmarks:\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "        # Define landmark indices for eyes + mouth regions\n",
    "        eye_indices = [33, 133, 362, 263]  # corners of both eyes\n",
    "        mouth_indices = [61, 291, 0, 17]   # mouth corners, chin, nose\n",
    "\n",
    "        def expand_bbox(indices, expand_x_ratio=0.5, expand_y_ratio=0.6):\n",
    "            xs = [int(face_landmarks.landmark[i].x * w) for i in indices]\n",
    "            ys = [int(face_landmarks.landmark[i].y * h) for i in indices]\n",
    "            x_min, x_max = min(xs), max(xs)\n",
    "            y_min, y_max = min(ys), max(ys)\n",
    "            box_w = x_max - x_min\n",
    "            box_h = y_max - y_min\n",
    "            # Expand the box\n",
    "            x_min = max(0, int(x_min - box_w * expand_x_ratio))\n",
    "            x_max = min(w, int(x_max + box_w * expand_x_ratio))\n",
    "            y_min = max(0, int(y_min - box_h * expand_y_ratio))\n",
    "            y_max = min(h, int(y_max + box_h * expand_y_ratio))\n",
    "            return x_min, y_min, x_max, y_max\n",
    "\n",
    "        # Get expanded regions\n",
    "        eye_x1, eye_y1, eye_x2, eye_y2 = expand_bbox(eye_indices,expand_x_ratio=0.5, expand_y_ratio=5)\n",
    "        mouth_x1, mouth_y1, mouth_x2, mouth_y2 = expand_bbox(mouth_indices, expand_x_ratio=0.8, expand_y_ratio=2)\n",
    "\n",
    "        eye_crop = img[eye_y1:eye_y2, eye_x1:eye_x2]\n",
    "        mouth_crop = img[mouth_y1:mouth_y2, mouth_x1:mouth_x2]\n",
    "\n",
    "        # Optional: save crops for inspection\n",
    "        cv2.imwrite(\"eye_region.jpg\", eye_crop)\n",
    "        cv2.imwrite(\"mouth_region.jpg\", mouth_crop)\n",
    "\n",
    "        # Run DeepFace analysis\n",
    "        eye_result = DeepFace.analyze(img_path=\"eye_region.jpg\", actions=['emotion'], enforce_detection=False)\n",
    "        mouth_result = DeepFace.analyze(img_path=\"mouth_region.jpg\", actions=['emotion'], enforce_detection=False)\n",
    "\n",
    "        print(\"ðŸ‘€ Eye Region Emotions:\", eye_result[0]['emotion'])\n",
    "        print(\"ðŸ‘„ Mouth Region Emotions:\", mouth_result[0]['emotion'])\n",
    "\n",
    "else:\n",
    "    print(\"No face detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52f6326d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any data file at /Users/renan/code/reneiji/non_verbal_comm/notebooks/RA_note/confidence_dataset/FacialConfidence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/renan/code/reneiji/non_verbal_comm/notebooks/RA_note/confidence_dataset/FacialConfidence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages/datasets/load.py:2062\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2057\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2058\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2059\u001b[0m )\n\u001b[1;32m   2061\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2062\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2075\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages/datasets/load.py:1782\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1781\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1782\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1795\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/non_verbal_comm/lib/python3.10/site-packages/datasets/load.py:1670\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1666\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1667\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1668\u001b[0m     )\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any data file at /Users/renan/code/reneiji/non_verbal_comm/notebooks/RA_note/confidence_dataset/FacialConfidence."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"/Users/renan/code/reneiji/non_verbal_comm/notebooks/RA_note/confidence_dataset/FacialConfidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc99c92",
   "metadata": {},
   "source": [
    "## Trying out with masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21244ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘€ Eye-Focused Emotions: {'angry': 0.3137337975203991, 'disgust': 64.55342173576355, 'fear': 18.41873675584793, 'happy': 1.471702754497528, 'sad': 4.76096048951149, 'surprise': 1.0638452135026455, 'neutral': 9.417599439620972}\n",
      "ðŸ‘„ Mouth-Focused Emotions: {'angry': 2.925314045428636e-14, 'disgust': 1.1485896445150053e-35, 'fear': 1.5336007138891326e-16, 'happy': 100.0, 'sad': 5.419644225589104e-11, 'surprise': 2.552502920138977e-06, 'neutral': 2.8781537157913928e-11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748310802.457964 2706939 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M3 Pro\n",
      "W0000 00:00:1748310802.460631 2764097 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748310802.469694 2764099 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "\n",
    "img_path = \"/Users/renan/code/reneiji/non_verbal_comm/notebooks/RA_note/Genuine_Smile.jpg\"\n",
    "img = cv2.imread(img_path)\n",
    "if img is None:\n",
    "    print(f\"âš  Failed to load image. Check path: {img_path}\")\n",
    "    exit()\n",
    "h, w, _ = img.shape\n",
    "rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "results = face_mesh.process(rgb_img)\n",
    "\n",
    "if results.multi_face_landmarks:\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "        # Define regions to mask\n",
    "        eye_indices = [33, 133, 362, 263]   # outer eye corners\n",
    "        mouth_indices = [61, 291, 0, 17]    # mouth corners, chin, nose\n",
    "\n",
    "        def get_bbox(indices, pad_ratio=0.3):\n",
    "            xs = [int(face_landmarks.landmark[i].x * w) for i in indices]\n",
    "            ys = [int(face_landmarks.landmark[i].y * h) for i in indices]\n",
    "            x_min, x_max = min(xs), max(xs)\n",
    "            y_min, y_max = min(ys), max(ys)\n",
    "            box_w = x_max - x_min\n",
    "            box_h = y_max - y_min\n",
    "            # Expand slightly\n",
    "            x_min = max(0, int(x_min - box_w * pad_ratio))\n",
    "            x_max = min(w, int(x_max + box_w * pad_ratio))\n",
    "            y_min = max(0, int(y_min - box_h * pad_ratio))\n",
    "            y_max = min(h, int(y_max + box_h * pad_ratio))\n",
    "            return x_min, y_min, x_max, y_max\n",
    "\n",
    "        eye_x1, eye_y1, eye_x2, eye_y2 = get_bbox(eye_indices)\n",
    "        mouth_x1, mouth_y1, mouth_x2, mouth_y2 = get_bbox(mouth_indices)\n",
    "\n",
    "        # Make copies of the original image\n",
    "        mask_eye = img.copy()\n",
    "        mask_mouth = img.copy()\n",
    "\n",
    "        blur_mouth = cv2.GaussianBlur(mask_eye[mouth_y1:mouth_y2, mouth_x1:mouth_x2], (51, 51), 0)\n",
    "        blur_eye = cv2.GaussianBlur(mask_mouth[0:eye_y2, :], (51, 51), 0)\n",
    "\n",
    "        # Mask out mouth + chin for eye-focused image\n",
    "        mask_eye[mouth_y1:mouth_y2, mouth_x1:mouth_x2] = blur_mouth\n",
    "\n",
    "        # Mask out eyes + forehead for mouth-focused image\n",
    "        mask_mouth[0:eye_y2, :] = blur_eye  # roughly top half\n",
    "\n",
    "        # Optional: save or visualize masked images\n",
    "        cv2.imwrite(\"masked_eye_focus.jpg\", mask_eye)\n",
    "        cv2.imwrite(\"masked_mouth_focus.jpg\", mask_mouth)\n",
    "\n",
    "        # Run DeepFace analysis\n",
    "        eye_result = DeepFace.analyze(img_path=\"masked_eye_focus.jpg\", actions=['emotion'], enforce_detection=False)\n",
    "        mouth_result = DeepFace.analyze(img_path=\"masked_mouth_focus.jpg\", actions=['emotion'], enforce_detection=False)\n",
    "\n",
    "        print(\"ðŸ‘€ Eye-Focused Emotions:\", eye_result[0]['emotion'])\n",
    "        print(\"ðŸ‘„ Mouth-Focused Emotions:\", mouth_result[0]['emotion'])\n",
    "\n",
    "else:\n",
    "    print(\"No face detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed3accf",
   "metadata": {},
   "source": [
    "## Blurrying + video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "312860cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748308318.237882 2672173 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M3 Pro\n",
      "W0000 00:00:1748308318.239703 2704677 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748308318.247964 2704674 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5\n",
    ")\n",
    "\n",
    "video_path = \"/Users/renan/code/reneiji/non_verbal_comm/notebooks/RA_note/monologue_video.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Define output writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('output_blur.mp4', fourcc, int(cap.get(cv2.CAP_PROP_FPS)),\n",
    "                      (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "            def get_bbox(indices, pad_ratio=0.3):\n",
    "                xs = [int(face_landmarks.landmark[i].x * w) for i in indices]\n",
    "                ys = [int(face_landmarks.landmark[i].y * h) for i in indices]\n",
    "                x_min, x_max = min(xs), max(xs)\n",
    "                y_min, y_max = min(ys), max(ys)\n",
    "                box_w = x_max - x_min\n",
    "                box_h = y_max - y_min\n",
    "                x_min = max(0, int(x_min - box_w * pad_ratio))\n",
    "                x_max = min(w, int(x_max + box_w * pad_ratio))\n",
    "                y_min = max(0, int(y_min - box_h * pad_ratio))\n",
    "                y_max = min(h, int(y_max + box_h * pad_ratio))\n",
    "                return x_min, y_min, x_max, y_max\n",
    "\n",
    "            # Example: blur mouth\n",
    "            mouth_indices = [61, 291, 0, 17]\n",
    "            mx1, my1, mx2, my2 = get_bbox(mouth_indices)\n",
    "            mouth_roi = frame[my1:my2, mx1:mx2]\n",
    "            blurred_mouth = cv2.GaussianBlur(mouth_roi, (51, 51), 0)\n",
    "            frame[my1:my2, mx1:mx2] = blurred_mouth\n",
    "\n",
    "            # Example: blur eyes (optional)\n",
    "            eye_indices = [33, 133, 362, 263]\n",
    "            ex1, ey1, ex2, ey2 = get_bbox(eye_indices)\n",
    "            eye_roi = frame[0:ey2, ex1:ex2]\n",
    "            blurred_eye = cv2.GaussianBlur(eye_roi, (51, 51), 0)\n",
    "            frame[0:ey2, ex1:ex2] = blurred_eye\n",
    "\n",
    "    # Write frame to output\n",
    "    out.write(frame)\n",
    "\n",
    "    # Optional: show preview\n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b025fb32",
   "metadata": {},
   "source": [
    "# Removing scenes where faces are not detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac08a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748392566.350694 3152210 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M3 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1748392566.364795 3426671 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Setup Mediapipe\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.6)\n",
    "\n",
    "# Open webcam (use 0 for default camera, or provide video path)\n",
    "cap = cv2.VideoCapture(0)  # replace 0 with \"input_video.mp4\" for file\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to RGB for Mediapipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_frame)\n",
    "\n",
    "    if results.detections:\n",
    "        for detection in results.detections:\n",
    "            # Get face bounding box\n",
    "            bbox = detection.location_data.relative_bounding_box\n",
    "            h, w, _ = frame.shape\n",
    "            x1 = int(bbox.xmin * w)\n",
    "            y1 = int(bbox.ymin * h)\n",
    "            x2 = x1 + int(bbox.width * w)\n",
    "            y2 = y1 + int(bbox.height * h)\n",
    "\n",
    "            # Ensure box is within frame\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(w, x2), min(h, y2)\n",
    "\n",
    "            face_img = frame[y1:y2, x1:x2]\n",
    "\n",
    "            try:\n",
    "                # Analyze emotion with DeepFace\n",
    "                result = DeepFace.analyze(face_img, actions=['emotion'], enforce_detection=False)\n",
    "                dominant_emotion = result[0]['dominant_emotion']\n",
    "\n",
    "                # Draw bounding box and emotion label\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, dominant_emotion, (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"DeepFace error: {e}\")\n",
    "\n",
    "    # Show live frame\n",
    "    cv2.imshow('Live Emotion Analyzer', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bff8d9",
   "metadata": {},
   "source": [
    "## Using emotion recognizer and heuristic approach to predict confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068df345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748331090.563067 3149569 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M3 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1748331090.564689 3149917 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1748331090.568680 3149569 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M3 Pro\n",
      "W0000 00:00:1748331090.569678 3149929 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748331090.573011 3149928 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1748331092.046464 3149932 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import mediapipe as mp\n",
    "\n",
    "# Setup Mediapipe\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.6)\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_frame)\n",
    "\n",
    "    label = \"No face\"\n",
    "    if results.detections:\n",
    "        for detection in results.detections:\n",
    "            # Get bounding box\n",
    "            bbox = detection.location_data.relative_bounding_box\n",
    "            h, w, _ = frame.shape\n",
    "            x1 = int(bbox.xmin * w)\n",
    "            y1 = int(bbox.ymin * h)\n",
    "            x2 = x1 + int(bbox.width * w)\n",
    "            y2 = y1 + int(bbox.height * h)\n",
    "            x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w, x2), min(h, y2)\n",
    "\n",
    "            face_img = frame[y1:y2, x1:x2]\n",
    "\n",
    "            try:\n",
    "                # Analyze emotion\n",
    "                result = DeepFace.analyze(face_img, actions=['emotion'], enforce_detection=False)\n",
    "                emotion = result[0]['dominant_emotion']\n",
    "\n",
    "                # Analyze landmarks\n",
    "                mesh_results = face_mesh.process(rgb_frame)\n",
    "                tension_score = 0\n",
    "                if mesh_results.multi_face_landmarks:\n",
    "                    landmarks = mesh_results.multi_face_landmarks[0].landmark\n",
    "\n",
    "                    # Mouth tightness\n",
    "                    top_lip = landmarks[13]\n",
    "                    bottom_lip = landmarks[14]\n",
    "                    lip_distance = abs(top_lip.y - bottom_lip.y)\n",
    "                    if lip_distance < 0.02:\n",
    "                        tension_score += 1  # tight mouth\n",
    "\n",
    "                    # Eyebrow raise\n",
    "                    left_brow = landmarks[65]\n",
    "                    left_eye = landmarks[159]\n",
    "                    brow_eye_gap = abs(left_brow.y - left_eye.y)\n",
    "                    if brow_eye_gap > 0.05:\n",
    "                        tension_score += 1  # raised brow\n",
    "\n",
    "                    # Eye openness\n",
    "                    left_upper = landmarks[159]\n",
    "                    left_lower = landmarks[145]\n",
    "                    eye_open = abs(left_upper.y - left_lower.y)\n",
    "                    if eye_open > 0.04:\n",
    "                        tension_score += 1  # wide eyes\n",
    "\n",
    "                    # Jaw clench (side-to-side width check)\n",
    "                    left_jaw = landmarks[234]\n",
    "                    right_jaw = landmarks[454]\n",
    "                    jaw_width = abs(left_jaw.x - right_jaw.x)\n",
    "                    if jaw_width < 0.35:\n",
    "                        tension_score += 1  # clenched jaw (narrower)\n",
    "\n",
    "                # Heuristic rule\n",
    "                if emotion in ['fear', 'surprise', 'disgust'] or tension_score >= 2:\n",
    "                    label = \"Nervous\"\n",
    "                elif emotion in ['happy', 'neutral'] and tension_score <= 1:\n",
    "                    label = \"Confident\"\n",
    "                else:\n",
    "                    label = \"Uncertain\"\n",
    "\n",
    "                # Draw box + label\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"{label} ({emotion}, tension={tension_score})\", (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"DeepFace error: {e}\")\n",
    "\n",
    "    # Show live video\n",
    "    cv2.imshow('Confidence/Nervousness Estimator', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e67852",
   "metadata": {},
   "source": [
    "# Using Pytorch for facial confidence estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdb08065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Dataset wrapper\n",
    "class FaceConfidenceDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.df.iloc[idx]['image']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "\n",
    "        # Ensure RGB\n",
    "        if isinstance(img, str):\n",
    "            img = Image.open(img).convert('RGB')\n",
    "        else:\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Step 2: Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to grayscale RGB\n",
    "    transforms.Resize((224, 224)),                # Resize to match ResNet input\n",
    "    transforms.ToTensor(),                        # Convert to tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],   # Normalize (ImageNet mean)\n",
    "                         [0.229, 0.224, 0.225])   # Normalize (ImageNet std)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2d4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.1: Get data set\n",
    "ds = load_dataset(\"march18/FacialConfidence\")\n",
    "df_train = pd.DataFrame.from_dict(ds['train'])\n",
    "df_test = pd.DataFrame.from_dict(ds['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6125a176",
   "metadata": {},
   "source": [
    "### Below is the first base model to predict confident face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69666a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.4308\n",
      "Epoch 2/5, Loss: 0.2816\n",
      "Epoch 3/5, Loss: 0.1649\n",
      "Epoch 4/5, Loss: 0.0888\n",
      "Epoch 5/5, Loss: 0.0635\n",
      "Test Accuracy: 0.8380\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Dataloaders\n",
    "train_dataset = FaceConfidenceDataset(df_train, transform)\n",
    "test_dataset = FaceConfidenceDataset(df_test, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Step 4: Model (pretrained ResNet18)\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)  # 2 classes: confident, non-confident\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Step 5: Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 5\n",
    "\n",
    "# Step 6: Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Step 7: Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47a5c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = models.resnet18(pretrained=False)\\nmodel.fc = nn.Linear(model.fc.in_features, 2)  # same output setup\\n\\nmodel.load_state_dict(torch.load('first_model.pth'))\\nmodel.to(device)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model\n",
    "# torch.save(model.state_dict(), 'first_model.pth')\n",
    "\n",
    "# Below is the code to load first_model.pth and make predictions\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)  # same output setup\n",
    "\n",
    "model.load_state_dict(torch.load('first_model.pth'))\n",
    "model.to(device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610bf579",
   "metadata": {},
   "source": [
    "### Second model training with higher epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95bc4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 Train Loss: 0.4328 Train Acc: 0.7927 Val Loss: 0.3685 Val Acc: 0.8373\n",
      "âœ… Best model saved at epoch 1 with val acc: 0.8373\n",
      "Epoch 2/15 Train Loss: 0.2816 Train Acc: 0.8811 Val Loss: 0.3915 Val Acc: 0.8267\n",
      "Epoch 3/15 Train Loss: 0.1636 Train Acc: 0.9351 Val Loss: 0.5023 Val Acc: 0.8292\n",
      "Epoch 4/15 Train Loss: 0.0852 Train Acc: 0.9686 Val Loss: 0.5520 Val Acc: 0.8321\n",
      "Epoch 5/15 Train Loss: 0.0265 Train Acc: 0.9919 Val Loss: 0.5519 Val Acc: 0.8508\n",
      "âœ… Best model saved at epoch 5 with val acc: 0.8508\n",
      "Epoch 6/15 Train Loss: 0.0095 Train Acc: 0.9981 Val Loss: 0.6337 Val Acc: 0.8536\n",
      "âœ… Best model saved at epoch 6 with val acc: 0.8536\n",
      "Epoch 7/15 Train Loss: 0.0054 Train Acc: 0.9989 Val Loss: 0.6768 Val Acc: 0.8465\n",
      "Epoch 8/15 Train Loss: 0.0033 Train Acc: 0.9994 Val Loss: 0.6537 Val Acc: 0.8532\n",
      "Epoch 9/15 Train Loss: 0.0019 Train Acc: 0.9996 Val Loss: 0.6775 Val Acc: 0.8537\n",
      "âœ… Best model saved at epoch 9 with val acc: 0.8537\n",
      "Epoch 10/15 Train Loss: 0.0015 Train Acc: 0.9996 Val Loss: 0.6974 Val Acc: 0.8540\n",
      "âœ… Best model saved at epoch 10 with val acc: 0.8540\n",
      "Epoch 11/15 Train Loss: 0.0015 Train Acc: 0.9995 Val Loss: 0.7435 Val Acc: 0.8564\n",
      "âœ… Best model saved at epoch 11 with val acc: 0.8564\n",
      "Epoch 12/15 Train Loss: 0.0014 Train Acc: 0.9996 Val Loss: 0.7196 Val Acc: 0.8544\n",
      "Epoch 13/15 Train Loss: 0.0013 Train Acc: 0.9995 Val Loss: 0.7182 Val Acc: 0.8550\n",
      "Epoch 14/15 Train Loss: 0.0010 Train Acc: 0.9996 Val Loss: 0.7250 Val Acc: 0.8553\n",
      "Epoch 15/15 Train Loss: 0.0010 Train Acc: 0.9995 Val Loss: 0.7233 Val Acc: 0.8536\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Datasets & Dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = FaceConfidenceDataset(df_train, transform)\n",
    "test_dataset = FaceConfidenceDataset(df_test, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss, Optimizer, Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "\n",
    "epochs = 15\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_preds, train_labels = [], []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds, val_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} \"\n",
    "          f\"Train Acc: {train_acc:.4f} \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} \"\n",
    "          f\"Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model (optional)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"âœ… Best model saved at epoch {epoch + 1} with val acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeffce8",
   "metadata": {},
   "source": [
    "## Loading the best model for detecting facial confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da4206ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate the model architecture\n",
    "model = models.resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)  # 2 classes: confident (0), non-confident (1)\n",
    "\n",
    "# Load saved weights\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location='cpu'))  # change to 'cuda' if using GPU\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28687ad",
   "metadata": {},
   "source": [
    "## Plotting confusion matrix and classification report for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f30be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Confident', 'Non-Confident'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix on Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4d9ed69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Confident       0.87      0.78      0.82      3007\n",
      "Non-Confident       0.85      0.91      0.88      4171\n",
      "\n",
      "     accuracy                           0.86      7178\n",
      "    macro avg       0.86      0.85      0.85      7178\n",
      " weighted avg       0.86      0.86      0.85      7178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(all_labels, all_preds, target_names=['Confident', 'Non-Confident'])\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b10c92e",
   "metadata": {},
   "source": [
    "## Simulating a real-time webcam feed with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbaeccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748394111.316820 3455154 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M3 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1748394111.318373 3465987 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n",
      "Error: name 'model' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Setup Mediapipe\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = models.resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.6)\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to RGB for Mediapipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_frame)\n",
    "\n",
    "    if results.detections:\n",
    "        for detection in results.detections:\n",
    "            bbox = detection.location_data.relative_bounding_box\n",
    "            h, w, _ = frame.shape\n",
    "            x1 = int(bbox.xmin * w)\n",
    "            y1 = int(bbox.ymin * h)\n",
    "            x2 = x1 + int(bbox.width * w)\n",
    "            y2 = y1 + int(bbox.height * h)\n",
    "\n",
    "            # Ensure box is within frame\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = min(w, x2), min(h, y2)\n",
    "\n",
    "            face_img = frame[y1:y2, x1:x2]\n",
    "\n",
    "            try:\n",
    "                # Analyze emotion with DeepFace\n",
    "                result = DeepFace.analyze(face_img, actions=['emotion'], enforce_detection=False)\n",
    "                dominant_emotion = result[0]['dominant_emotion']\n",
    "\n",
    "                # Predict confidence with your model\n",
    "                pil_face = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n",
    "                input_tensor = transform(pil_face).unsqueeze(0).to(device)\n",
    "                output = model(input_tensor)\n",
    "                _, pred = torch.max(output, 1)\n",
    "                confidence_label = 'Confident' if pred.item() == 0 else 'Non-Confident'\n",
    "\n",
    "                # Combine labels\n",
    "                combined_label = f\"{dominant_emotion}, {confidence_label}\"\n",
    "\n",
    "                # Draw bounding box and labels\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, combined_label, (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "    cv2.imshow('Live Emotion + Confidence Analyzer', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c246323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "non_verbal_comm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
